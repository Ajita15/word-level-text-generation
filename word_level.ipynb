{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data=open(\"wonderland3.txt\",encoding='utf-8').read()\n",
    "data=data.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "144348"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2=data.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeffchapter i. down the rabbit-hole  alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was rea'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data2=data2.replace('\\ufeff','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chapter i. down the rabbit-hole  alice was beginning to get very tired of sitting by her sister on the bank, and of having nothing to do: once or twice she had peeped into the book her sister was read'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def clean_data(data):\n",
    "    #data=data.replace('--',' ')\n",
    "    tokens=data.split()\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    tokens=[w.translate(table) for w in tokens]\n",
    "    tokens=[w for w in tokens if w.isalpha()]\n",
    "    tokens=[w.lower() for w in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'down',\n",
       " 'the',\n",
       " 'rabbithole',\n",
       " 'alice',\n",
       " 'was',\n",
       " 'beginning',\n",
       " 'to',\n",
       " 'get',\n",
       " 'very',\n",
       " 'tired',\n",
       " 'of',\n",
       " 'sitting',\n",
       " 'by',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'on',\n",
       " 'the',\n",
       " 'bank',\n",
       " 'and',\n",
       " 'of',\n",
       " 'having',\n",
       " 'nothing',\n",
       " 'to',\n",
       " 'do',\n",
       " 'once',\n",
       " 'or',\n",
       " 'twice',\n",
       " 'she',\n",
       " 'had',\n",
       " 'peeped',\n",
       " 'into',\n",
       " 'the',\n",
       " 'book',\n",
       " 'her',\n",
       " 'sister',\n",
       " 'was',\n",
       " 'reading',\n",
       " 'but',\n",
       " 'it',\n",
       " 'had',\n",
       " 'no',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'conversations',\n",
       " 'in',\n",
       " 'it',\n",
       " 'what',\n",
       " 'is',\n",
       " 'the',\n",
       " 'use',\n",
       " 'of',\n",
       " 'a',\n",
       " 'thought',\n",
       " 'alice',\n",
       " 'pictures',\n",
       " 'or',\n",
       " 'so',\n",
       " 'she',\n",
       " 'was',\n",
       " 'considering',\n",
       " 'in',\n",
       " 'her',\n",
       " 'own',\n",
       " 'mind',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'she',\n",
       " 'could',\n",
       " 'for',\n",
       " 'the',\n",
       " 'hot',\n",
       " 'day',\n",
       " 'made',\n",
       " 'her',\n",
       " 'feel',\n",
       " 'very',\n",
       " 'sleepy',\n",
       " 'and',\n",
       " 'stupid',\n",
       " 'whether',\n",
       " 'the',\n",
       " 'pleasure',\n",
       " 'of',\n",
       " 'making',\n",
       " 'a',\n",
       " 'daisychain',\n",
       " 'would',\n",
       " 'be',\n",
       " 'worth',\n",
       " 'the',\n",
       " 'trouble',\n",
       " 'of',\n",
       " 'getting',\n",
       " 'up',\n",
       " 'and',\n",
       " 'picking',\n",
       " 'the',\n",
       " 'daisies',\n",
       " 'when',\n",
       " 'suddenly',\n",
       " 'a',\n",
       " 'white',\n",
       " 'rabbit',\n",
       " 'with',\n",
       " 'pink',\n",
       " 'eyes',\n",
       " 'ran',\n",
       " 'close',\n",
       " 'by',\n",
       " 'her',\n",
       " 'there',\n",
       " 'was',\n",
       " 'nothing',\n",
       " 'so',\n",
       " 'very',\n",
       " 'remarkable',\n",
       " 'in',\n",
       " 'that',\n",
       " 'nor',\n",
       " 'did',\n",
       " 'alice',\n",
       " 'think',\n",
       " 'it',\n",
       " 'so',\n",
       " 'very',\n",
       " 'much',\n",
       " 'out',\n",
       " 'of',\n",
       " 'the',\n",
       " 'way',\n",
       " 'to',\n",
       " 'hear',\n",
       " 'the',\n",
       " 'rabbit',\n",
       " 'say',\n",
       " 'to',\n",
       " 'itself',\n",
       " 'dear',\n",
       " 'oh',\n",
       " 'dear',\n",
       " 'i',\n",
       " 'shall',\n",
       " 'be',\n",
       " 'when',\n",
       " 'she',\n",
       " 'thought',\n",
       " 'it',\n",
       " 'over',\n",
       " 'afterwards',\n",
       " 'it',\n",
       " 'occurred',\n",
       " 'to',\n",
       " 'her',\n",
       " 'that',\n",
       " 'she',\n",
       " 'ought',\n",
       " 'to',\n",
       " 'have',\n",
       " 'wondered',\n",
       " 'at',\n",
       " 'this',\n",
       " 'but',\n",
       " 'at',\n",
       " 'the',\n",
       " 'time',\n",
       " 'it',\n",
       " 'all',\n",
       " 'seemed',\n",
       " 'quite',\n",
       " 'natural',\n",
       " 'but',\n",
       " 'when',\n",
       " 'the',\n",
       " 'rabbit',\n",
       " 'actually',\n",
       " 'took',\n",
       " 'a',\n",
       " 'watch',\n",
       " 'out',\n",
       " 'of',\n",
       " 'its',\n",
       " 'waistcoatpocket',\n",
       " 'and',\n",
       " 'looked',\n",
       " 'at',\n",
       " 'it',\n",
       " 'and',\n",
       " 'then',\n",
       " 'hurried',\n",
       " 'on',\n",
       " 'alice',\n",
       " 'started',\n",
       " 'to',\n",
       " 'her',\n",
       " 'feet',\n",
       " 'for',\n",
       " 'it',\n",
       " 'flashed']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaned=clean_data(data)\n",
    "data_cleaned[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23623\n",
      "2526\n"
     ]
    }
   ],
   "source": [
    "print(len(data_cleaned))\n",
    "print(len(set(data_cleaned)))\n",
    "length=len(data_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#creating a sequence of 50 input word and one output word\n",
    "seq=[]\n",
    "seq_length=51\n",
    "for i in range(seq_length,length):\n",
    "    shrt_seq=data_cleaned[i-seq_length:i]\n",
    "    line=' '.join(shrt_seq)\n",
    "    seq.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save tokens to file i.e one seq per line\n",
    "def save_doc(lines,filename):\n",
    "    data='\\n'.join(lines)\n",
    "    f=open(filename,\"w\")\n",
    "    f.write(data)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23572"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "save_doc(seq,\"wonderland_seq.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "tokenizer=Tokenizer()\n",
    "tokenizer.fit_on_texts(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines=tokenizer.texts_to_sequences(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2526"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vocab_size=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sequences=np.array(lines)\n",
    "X,y=sequences[:,:-1],sequences[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "y=to_categorical(y,num_classes=vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "seq_length=X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense,Dropout,LSTM,Input,Embedding\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Embedding(vocab_size,50,input_length=seq_length))\n",
    "model.add(LSTM(100,return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100,activation='relu'))\n",
    "model.add(Dense(vocab_size,activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 50)            126350    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 50, 100)           60400     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100)               80400     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2527)              255227    \n",
      "=================================================================\n",
      "Total params: 532,477\n",
      "Trainable params: 532,477\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile model\n",
    "model.compile(loss='categorical_crossentropy',metrics=['accuracy'],optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "23572/23572 [==============================] - 95s 4ms/step - loss: 6.2754 - acc: 0.0671\n",
      "Epoch 2/100\n",
      "23572/23572 [==============================] - 97s 4ms/step - loss: 5.9583 - acc: 0.0680\n",
      "Epoch 3/100\n",
      "23572/23572 [==============================] - 90s 4ms/step - loss: 5.7968 - acc: 0.0692\n",
      "Epoch 4/100\n",
      "23572/23572 [==============================] - 97s 4ms/step - loss: 5.6337 - acc: 0.0726\n",
      "Epoch 5/100\n",
      "23572/23572 [==============================] - 85s 4ms/step - loss: 5.5411 - acc: 0.0769\n",
      "Epoch 6/100\n",
      "23572/23572 [==============================] - 86s 4ms/step - loss: 5.4631 - acc: 0.0834\n",
      "Epoch 7/100\n",
      "23572/23572 [==============================] - 93s 4ms/step - loss: 5.3947 - acc: 0.0870\n",
      "Epoch 8/100\n",
      "23572/23572 [==============================] - 82s 3ms/step - loss: 5.3342 - acc: 0.0893\n",
      "Epoch 9/100\n",
      "23572/23572 [==============================] - 80s 3ms/step - loss: 5.2741 - acc: 0.0896\n",
      "Epoch 10/100\n",
      "23572/23572 [==============================] - 81s 3ms/step - loss: 5.2102 - acc: 0.0911\n",
      "Epoch 11/100\n",
      "23572/23572 [==============================] - 82s 3ms/step - loss: 5.1324 - acc: 0.0995\n",
      "Epoch 12/100\n",
      "23572/23572 [==============================] - 82s 3ms/step - loss: 5.0603 - acc: 0.1089\n",
      "Epoch 13/100\n",
      "23572/23572 [==============================] - 84s 4ms/step - loss: 4.9936 - acc: 0.1138\n",
      "Epoch 14/100\n",
      "23572/23572 [==============================] - 88s 4ms/step - loss: 4.9353 - acc: 0.1220\n",
      "Epoch 15/100\n",
      "23572/23572 [==============================] - 85s 4ms/step - loss: 4.8836 - acc: 0.1237\n",
      "Epoch 16/100\n",
      "23572/23572 [==============================] - 87s 4ms/step - loss: 4.8365 - acc: 0.1265\n",
      "Epoch 17/100\n",
      "23572/23572 [==============================] - 96s 4ms/step - loss: 4.7892 - acc: 0.1290\n",
      "Epoch 18/100\n",
      "23572/23572 [==============================] - 91s 4ms/step - loss: 4.7431 - acc: 0.1334\n",
      "Epoch 19/100\n",
      "23572/23572 [==============================] - 92s 4ms/step - loss: 4.7031 - acc: 0.1365\n",
      "Epoch 20/100\n",
      "23572/23572 [==============================] - 96s 4ms/step - loss: 4.6632 - acc: 0.1414\n",
      "Epoch 21/100\n",
      "23572/23572 [==============================] - 84s 4ms/step - loss: 4.6368 - acc: 0.1433\n",
      "Epoch 22/100\n",
      "23572/23572 [==============================] - 87s 4ms/step - loss: 4.5959 - acc: 0.1462\n",
      "Epoch 23/100\n",
      "23572/23572 [==============================] - 87s 4ms/step - loss: 4.5600 - acc: 0.1486\n",
      "Epoch 24/100\n",
      "23572/23572 [==============================] - 84s 4ms/step - loss: 4.5196 - acc: 0.1525\n",
      "Epoch 25/100\n",
      "23572/23572 [==============================] - 84s 4ms/step - loss: 4.4854 - acc: 0.1547\n",
      "Epoch 26/100\n",
      "23572/23572 [==============================] - 83s 4ms/step - loss: 4.4487 - acc: 0.1568\n",
      "Epoch 27/100\n",
      "23572/23572 [==============================] - 82s 3ms/step - loss: 4.4732 - acc: 0.1549\n",
      "Epoch 28/100\n",
      "23572/23572 [==============================] - 82s 3ms/step - loss: 4.7543 - acc: 0.1361\n",
      "Epoch 29/100\n",
      "23572/23572 [==============================] - 82s 3ms/step - loss: 4.5119 - acc: 0.1509\n",
      "Epoch 30/100\n",
      "23572/23572 [==============================] - 83s 4ms/step - loss: 4.4327 - acc: 0.1563\n",
      "Epoch 31/100\n",
      "23572/23572 [==============================] - 37s 2ms/step - loss: 4.3749 - acc: 0.1607\n",
      "Epoch 32/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.3284 - acc: 0.1627\n",
      "Epoch 33/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.2827 - acc: 0.1648\n",
      "Epoch 34/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.2564 - acc: 0.1670\n",
      "Epoch 35/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.2188 - acc: 0.1702\n",
      "Epoch 36/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.1798 - acc: 0.1722\n",
      "Epoch 37/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.1386 - acc: 0.1743\n",
      "Epoch 38/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.0961 - acc: 0.1779\n",
      "Epoch 39/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.0594 - acc: 0.1792\n",
      "Epoch 40/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 4.0232 - acc: 0.1800\n",
      "Epoch 41/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.9789 - acc: 0.1869\n",
      "Epoch 42/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.9363 - acc: 0.1881\n",
      "Epoch 43/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.9005 - acc: 0.1919\n",
      "Epoch 44/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.8670 - acc: 0.1950\n",
      "Epoch 45/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.8490 - acc: 0.1970\n",
      "Epoch 46/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.8059 - acc: 0.1992\n",
      "Epoch 47/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.7697 - acc: 0.2008\n",
      "Epoch 48/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.7292 - acc: 0.2068\n",
      "Epoch 49/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.6894 - acc: 0.2103\n",
      "Epoch 50/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.6627 - acc: 0.2135\n",
      "Epoch 51/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.6408 - acc: 0.2152\n",
      "Epoch 52/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.6310 - acc: 0.2196\n",
      "Epoch 53/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.6060 - acc: 0.2242\n",
      "Epoch 54/100\n",
      "23572/23572 [==============================] - 28s 1ms/step - loss: 3.5661 - acc: 0.2271\n",
      "Epoch 55/100\n",
      "23572/23572 [==============================] - 28s 1ms/step - loss: 3.5186 - acc: 0.2310\n",
      "Epoch 56/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.5293 - acc: 0.2311\n",
      "Epoch 57/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.5113 - acc: 0.2338\n",
      "Epoch 58/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.4911 - acc: 0.2373\n",
      "Epoch 59/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.4315 - acc: 0.2417\n",
      "Epoch 60/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.4053 - acc: 0.2449\n",
      "Epoch 61/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.5572 - acc: 0.2312\n",
      "Epoch 62/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.3946 - acc: 0.2498\n",
      "Epoch 63/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.3686 - acc: 0.2518\n",
      "Epoch 64/100\n",
      "23572/23572 [==============================] - 30s 1ms/step - loss: 3.4391 - acc: 0.2430\n",
      "Epoch 65/100\n",
      "23572/23572 [==============================] - 28s 1ms/step - loss: 3.3824 - acc: 0.2514\n",
      "Epoch 66/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.2988 - acc: 0.2606\n",
      "Epoch 67/100\n",
      "23572/23572 [==============================] - 28s 1ms/step - loss: 3.2934 - acc: 0.2621\n",
      "Epoch 68/100\n",
      "23572/23572 [==============================] - 29s 1ms/step - loss: 3.1874 - acc: 0.2729\n",
      "Epoch 69/100\n",
      "23572/23572 [==============================] - 29s 1ms/step - loss: 3.1804 - acc: 0.2770\n",
      "Epoch 70/100\n",
      "23572/23572 [==============================] - 27s 1ms/step - loss: 3.1426 - acc: 0.2836\n",
      "Epoch 71/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0946 - acc: 0.2895\n",
      "Epoch 72/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0613 - acc: 0.2951\n",
      "Epoch 73/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.2031 - acc: 0.2798\n",
      "Epoch 74/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.2654 - acc: 0.2705\n",
      "Epoch 75/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.2046 - acc: 0.2762\n",
      "Epoch 76/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.1472 - acc: 0.2844\n",
      "Epoch 77/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0986 - acc: 0.2919\n",
      "Epoch 78/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0524 - acc: 0.2969\n",
      "Epoch 79/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0176 - acc: 0.3060\n",
      "Epoch 80/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0016 - acc: 0.3089\n",
      "Epoch 81/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0445 - acc: 0.3052\n",
      "Epoch 82/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0632 - acc: 0.2987\n",
      "Epoch 83/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0199 - acc: 0.3074\n",
      "Epoch 84/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.9866 - acc: 0.3113\n",
      "Epoch 85/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.8906 - acc: 0.3276\n",
      "Epoch 86/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.9050 - acc: 0.3239\n",
      "Epoch 87/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0827 - acc: 0.2969\n",
      "Epoch 88/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 3.0057 - acc: 0.3068\n",
      "Epoch 89/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.9559 - acc: 0.3169\n",
      "Epoch 90/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.9195 - acc: 0.3208\n",
      "Epoch 91/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.8853 - acc: 0.3289\n",
      "Epoch 92/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.8511 - acc: 0.3350\n",
      "Epoch 93/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.8226 - acc: 0.3410\n",
      "Epoch 94/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.7909 - acc: 0.3449\n",
      "Epoch 95/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.7643 - acc: 0.3531\n",
      "Epoch 96/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.7361 - acc: 0.3552\n",
      "Epoch 97/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.7060 - acc: 0.3625\n",
      "Epoch 98/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.6746 - acc: 0.3671\n",
      "Epoch 99/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.6454 - acc: 0.3740\n",
      "Epoch 100/100\n",
      "23572/23572 [==============================] - 26s 1ms/step - loss: 2.6190 - acc: 0.3773\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2166273bf98>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,y,batch_size=128,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred=model.predict(X)\n",
    "y_pred2=np.argmax(y_pred,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y2=np.argmax(y,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23572,)\n",
      "(23572,)\n"
     ]
    }
   ],
   "source": [
    "print(y2.shape)\n",
    "print(y_pred2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4140081452570847\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y2,y_pred2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_idx=tokenizer.word_index\n",
    "idx_to_word=dict((word_to_idx[c],c) for c in word_to_idx.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate sequences\n",
    "import sys\n",
    "def generate_text(seed_text):\n",
    "    for i in range(100):\n",
    "        x_inp=np.reshape(seed_text,(1,seed_text.shape[0]))\n",
    "        y_pred=model.predict(x_inp)\n",
    "        idx=np.argmax(y_pred,axis=1)\n",
    "        sys.stdout.write(idx_to_word[idx[0]])\n",
    "        sys.stdout.write(\" \")\n",
    "        seed_text=np.concatenate([seed_text,idx],axis=0)\n",
    "        seed_text=seed_text[1:len(seed_text)]\n",
    "    print(\"\\nDone\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faintly came carried on the breeze that followed them the melancholy words of the eeevening beautiful beautiful chapter xi who stole the tarts the king and queen of hearts were seated on their throne when they arrived with a great crowd assembled about themall sorts of little birds and beasts\n",
      "as she went on again and she tried to curtsey as she went on again and then she went on growing and then she went on in a helpless little juror it was bill i might not escape beginning to ask them the arch got to the other side of the reedsthe of play myself and repeat it i know that she had not gone to begin again wish i written down the dormouse denied you invented it would happen the dormouse of the eeevening beautiful beautiful soup soup of history and looked up and walking off to usurpation and \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "seed=np.random.randint(0,len(X)-1)\n",
    "seed_text=X[seed]\n",
    "line=[idx_to_word[idx] for idx in seed_text]\n",
    "print(' '.join(line))\n",
    "generate_text(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "come to the beginning alice ventured to ask we change the the march hare interrupted yawning getting tired of this i vote the young lady tells us a afraid i know said alice rather alarmed at the proposal the dormouse they both cried up and they pinched it on both\n",
      "sides of the classics master of the busy farmyardwhile the lowing of the party said to herself imagine its crown to the company and she went on again and she tried to curtsey as she could not help the dormouse had closed its time in a large hurry their backs was over in a thick party said the gryphon and the jury was thatched with fur i know what i will do to be lost permitted to fix on the dormouse of the eeevening beautiful beautiful chapter xi who were times close to the knave of hearts and she walked \n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "seed=np.random.randint(0,len(X)-1)\n",
    "seed_text=X[seed]\n",
    "line=[idx_to_word[idx] for idx in seed_text]\n",
    "print(' '.join(line))\n",
    "generate_text(seed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.save(\"word_level_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:Anaconda3]",
   "language": "python",
   "name": "conda-env-Anaconda3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
